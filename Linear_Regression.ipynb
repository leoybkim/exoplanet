{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb4eb0fa-ebe4-4f2e-b9ac-8784f5bac6b2",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "After exploring the Exoplanet dataset, I noticed that a considerable number of planets were lacking data points for their effective temperature and inclination, with 1396 and 1286 planets missing each, respectively.\n",
    "\n",
    "While linear regression method can be used for filling in missing data, it should be done with caution when a multitude of other variables also have missing values. This is because correlations between variables can become unstable and result in further complications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3530be6-a874-414d-9145-93c1da696946",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import math\n",
    "\n",
    "df = pd.read_csv('exoplanet_dataset.csv')\n",
    "\n",
    "# Drop the uncertainties and limit columns to simplify the data\n",
    "df = df.drop([col_name for col_name in df if col_name.endswith(('err1', 'err2', 'lim'))], axis=1)\n",
    "\n",
    "# Update the data type int64 into bool on boolean columns as documented in the column definition\n",
    "bool_col_names = ['rv_flag', 'pul_flag', 'ptv_flag', 'tran_flag', 'ast_flag', 'obm_flag', 'micro_flag', 'etv_flag', 'ima_flag', 'dkin_flag', 'ttv_flag', 'cb_flag']\n",
    "df[bool_col_names] = df[bool_col_names].astype(bool)\n",
    "\n",
    "col_to_remove = list((df.isnull().sum() / len(df)).loc[lambda p : p > 0.3].sort_values(ascending=False).index) # Drop columns with missing data >= 30%\n",
    "df = df.drop(col_to_remove, axis=1)\n",
    "df = df.drop(['pl_rade', 'pl_bmasse'], axis=1) # Drop highly correlated value\n",
    "df = df.drop(['discoverymethod'], axis=1) # Use one-hot encoded columns\n",
    "df = df.drop(['pl_name', 'hostname', 'disc_year', 'disc_locale', 'disc_facility', 'disc_telescope', 'disc_instrument', 'pl_bmassprov', 'st_metratio', 'ra_reflink'], axis=1) # Drop non numeric fields\n",
    "df = df.dropna() # Drop nan values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42e1d42-0aee-4d61-9060-74ed99ccf4a5",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "\n",
    "Standardization rescales the data so that the dataset has a mean of $\\mu = 0$ and standard deviation of $\\sigma = 1$.\n",
    "\n",
    "$${x_{new}}^{(i)} = {{(x^{(i)} - \\mu)} \\over \\sigma}$$\n",
    "\n",
    "## Normalization\n",
    "\n",
    "Normalization rescales the data so that each of the value falls between 0 and 1.\n",
    "\n",
    "$${x_{new}}^{(i)} = {{(x^{(i)} - x_{min})} \\over {(x_{max} - x_{min})}}$$ \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "Since most of the planetary features have different ranges, they should be transformed to a common scale. For example, `pl_eqt` is measured in Kelvins ranging from 30 ~ 4100 in value whereas `pl_bmassj` is measured in Jupiter mass ranging from as small as 6e-05 to 700s. Since the temperature variable has a bigger range, it will outweigh the mass variable due to its larger value but it shouldn't imply that temperature value is more important predictor for the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f615a4ac-76dc-423c-9452-fe3d2e5edf3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df[['pl_bmassj', 'pl_orbsmax', 'pl_eqt', 'st_mass', 'st_lum', 'st_logg']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30758444-a4f7-410a-bb4b-c2c3b157bb9d",
   "metadata": {},
   "source": [
    "## Split dataset into Train and Test\n",
    "\n",
    "Among the planets that have the temperature data, split the dataset into two groups: train(70%) and test(30%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5043739d-5bb4-4b50-a290-07bcc41cb97e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1993, 5)\n",
      "y_train shape: (1993,)\n",
      "X_test shape: (855, 5)\n",
      "y_test shape: (855,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "temp_df = df[df['pl_eqt'].notnull()]\n",
    "temp_pred_df = df[df['pl_eqt'].isnull()]\n",
    "y = temp_df['pl_eqt']\n",
    "X = temp_df.drop('pl_eqt', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # random state is used to get the same output each time\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2376da-fd53-451c-889b-00577e068063",
   "metadata": {},
   "source": [
    "## Compute cost\n",
    "\n",
    "The model function for linear regression which is a function that maps from `X` (planet parameters) to `y` (planet effective temperature (K)) is represented as: $$f_{W,b}(X) = WX + b$$\n",
    "To train this linear regression model, you need to find the best $(W,b)$ parameters that fit your dataset.\n",
    "To compare how a one parameter $(W,b)$ is better or worse than another is by evaluating it with a cost function $J(W,b)$.\n",
    "\n",
    "\n",
    "The cost function is defined as: $$J(W,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{W,b}(X^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "where $m$ is the number of training examples, $f_{W,b}(X^{(i)})$ is the model's prediction of the planet's effective temperature and $y^{(i)}$ is the actual effective temperature from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af1ae3cd-d19e-4e47-a88e-182d6733f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, W, b): \n",
    "    \"\"\"\n",
    "    Computes the cost function for multi-variable linear regression.\n",
    "    \n",
    "    Arguments:\n",
    "    X: input matrix of shape (m, n) (Planet parameters) \n",
    "    y: output vector of shape (m, 1) (Actual effective temperature of the planet)\n",
    "    W: weight vector of shape (1, n)\n",
    "    b: bias scalar\n",
    "    \n",
    "    Returns\n",
    "    cost -- scalar value of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0] # number of training examples\n",
    "    f_Wb = np.dot(X, W.T) + b  # predicted output values\n",
    "    loss = (f_Wb - y)**2  # squared error \n",
    "    cost = np.sum(loss) / (2 * m)  # compute cost using mean squared error\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7784d88e-2114-4e0c-9852-ee2fb5702d84",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "The parameter that fits the data best will have the smallest cost $J(W,b)$. Gradient descent is used to find that smallest cost by stepping closer to the optimal value. \n",
    "The algorithm for gradient descent is:\n",
    "$$\\begin{align*}& \\text{repeat until convergence:} \\; \\lbrace \\newline\\; \n",
    "& \\phantom {0000} b := b -  \\alpha \\frac{\\partial J(W,b)}{\\partial b} \\newline\\; \n",
    "& \\phantom {0000} W := W -  \\alpha \\frac{\\partial J(W,b)}{\\partial W} \\; \\newline \n",
    "& \\rbrace\\end{align*}$$\n",
    "\n",
    "where parameters $W$, $b$ are both updated simultaniously and where:\n",
    "$$\\frac{\\partial J(W,b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{W,b}(X^{(i)}) - y^{(i)})$$\n",
    "\n",
    "$$\\frac{\\partial J(W,b)}{\\partial W}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{W,b}(X^{(i)}) -y^{(i)})X^{(i)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b4b2a66-f5b9-459a-9779-61dacc59d670",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, W, b): \n",
    "    \"\"\"\n",
    "    Compute the gradient of the cost function for multi-variable linear regression.\n",
    "\n",
    "    Arguments:\n",
    "    X: input matrix of shape (m, n) (Planet parameters) \n",
    "    y: output vector of shape (m, 1) (Actual effective temperature of the planet)\n",
    "    W: weight vector of shape (1, n)\n",
    "    b: bias scalar\n",
    "    \n",
    "    Returns\n",
    "    dj_dW: The gradient of the cost with respect to the parameters W\n",
    "    dj_db: The gradient of the cost with respect to the parameter b     \n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]  # number of training examples\n",
    "    f_Wb = np.dot(X, W.T) + b  # predicted output values\n",
    "    diff = f_Wb - y  # difference between predicted and true output values\n",
    "\n",
    "    dj_dW = np.dot(diff.T, X) / m  # gradient with respect to the weights\n",
    "    dj_db = np.sum(diff) / m  # gradient with respect to the bias\n",
    "\n",
    "    return dj_dW, dj_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf183847-ef57-4815-8c40-4a8c62331de8",
   "metadata": {},
   "source": [
    "## Batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaff96ad-7257-4b87-9d08-c3ec69440aec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, W_in, b_in, cost_function, gradient_function, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent\n",
    "\n",
    "    Arguments:\n",
    "    X: input matrix of shape (m, n)\n",
    "    y: output vector of shape (m, 1)\n",
    "    W_in: initial weight vector of shape (1, n)\n",
    "    b_in: initial bias scalar\n",
    "    cost_function: function to compute the cost given X, y, W, and b\n",
    "    gradient_function: function to compute the gradient of the cost function given X, y, W, and b\n",
    "    alpha: learning rate\n",
    "    num_iters: number of iterations to run gradient descent\n",
    "\n",
    "    Returns:\n",
    "    W: optimal weight vector of shape (1, n)\n",
    "    b: optimal bias scalar\n",
    "    \"\"\"\n",
    "\n",
    "    # An array to store cost J and w's at each iteration â€” primarily for graphing later\n",
    "    J_history = []\n",
    "    W_history = []\n",
    "    W = copy.deepcopy(W_in)  # Avoid modifying global w within function\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_dW, dj_db = gradient_function(X, y, W, b)  \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        W = W - alpha * dj_dW               \n",
    "        b = b - alpha * dj_db               \n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            cost = cost_function(X, y, W, b)\n",
    "            J_history.append(cost)\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0:\n",
    "            W_history.append(W)\n",
    "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")\n",
    "        \n",
    "    return W, b, J_history, W_history # Return W and J,W history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9eee2c6e-4e96-439b-9195-e9ceeeb6bb65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.4472135954999579 0.4472135954999579\n",
      "-0.4434834268503958 0.44633775991168917\n",
      "-0.0062474354345094275 0.2560451467545742\n"
     ]
    }
   ],
   "source": [
    "m,n = X.shape # m is the number of samples and n is the number of features\n",
    "\n",
    "# Xavier Initialization\n",
    "lower, upper = -(1.0 / math.sqrt(n)), (1.0 / math.sqrt(n))\n",
    "numbers = np.random.rand(1000)\n",
    "scaled = lower + numbers * (upper - lower)\n",
    "initial_W = np.random.uniform(lower, upper, size=(1,n))\n",
    "initial_b = 0\n",
    "\n",
    "# gradient descent settings\n",
    "iterations = 1500\n",
    "alpha = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9014744-a925-4a97-941b-db6a647c66f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 1014361752.30   \n",
      "Iteration  150: Cost 188557078.77   \n",
      "Iteration  300: Cost 175445955.71   \n",
      "Iteration  450: Cost 164845270.00   \n",
      "Iteration  600: Cost 154899956.85   \n",
      "Iteration  750: Cost 145565039.46   \n",
      "Iteration  900: Cost 136802229.66   \n",
      "Iteration 1050: Cost 128575718.70   \n",
      "Iteration 1200: Cost 120852002.90   \n",
      "Iteration 1350: Cost 113599729.95   \n",
      "W,b found by gradient descent: [[ -1.1168913   -0.68849723  -2.33995544   0.11104437 -11.98049429]\n",
      " [ -1.24060069  -0.7282982   -2.6310364    0.12711716 -13.29150032]\n",
      " [ -3.46736983  -1.44471569  -7.87049366   0.41642736 -36.88960877]\n",
      " ...\n",
      " [ -0.61667504  -0.52756287  -1.16297591   0.0460544   -6.67946993]\n",
      " [  0.73337097  -0.09321314   2.01360325  -0.12934864   7.62759582]\n",
      " [ -2.66056942  -1.18514413  -5.97213958   0.31160482 -28.33956947]] 913.7926365034399\n"
     ]
    }
   ],
   "source": [
    "W,b,J_history,W_history = gradient_descent(X_train, y_train, initial_W, initial_b, compute_cost, compute_gradient, alpha, iterations)\n",
    "print(\"W,b found by gradient descent:\", W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7d5b14a-b3ab-43ac-9708-22d39693fdfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(X, W, b):\n",
    "    \"\"\"\n",
    "    Predicts the target variable given the input features and learned parameters.\n",
    "\n",
    "    Parameters:\n",
    "    X: input matrix of shape (m, n)\n",
    "    W: weight vector of shape (n, 1)\n",
    "    b: bias scalar\n",
    "\n",
    "    Returns:\n",
    "    y_pred: predicted targe variable of shape shape (m, 1).\n",
    "    \"\"\"\n",
    "    y_pred = np.dot(X, W.T) + b\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7ca3df5-4ef7-49ef-b5ad-1b900ed722b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "#Compute accuracy on our training set\n",
    "p = predict(X_train, W,b)\n",
    "print('Train Accuracy: %f'%(np.mean(p == y_train) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab23e5d9-44d8-4513-ad9d-8d1c4a367dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exo (venv)",
   "language": "python",
   "name": "exo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
