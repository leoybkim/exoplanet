{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb4eb0fa-ebe4-4f2e-b9ac-8784f5bac6b2",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "After exploring the Exoplanet dataset, I noticed that a considerable number of planets were lacking data points for their effective temperature and inclination, with 1396 and 1286 planets missing each, respectively.\n",
    "\n",
    "While linear regression method can be used for filling in missing data, it should be done with caution when a multitude of other variables also have missing values. This is because correlations between variables can become unstable and result in further complications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3530be6-a874-414d-9145-93c1da696946",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import math\n",
    "\n",
    "df = pd.read_csv('exoplanet_dataset.csv')\n",
    "\n",
    "# Drop the uncertainties and limit columns to simplify the data\n",
    "df = df.drop([col_name for col_name in df if col_name.endswith(('err1', 'err2', 'lim'))], axis=1)\n",
    "\n",
    "# Update the data type int64 into bool on boolean columns as documented in the column definition\n",
    "bool_col_names = ['rv_flag', 'pul_flag', 'ptv_flag', 'tran_flag', 'ast_flag', 'obm_flag', 'micro_flag', 'etv_flag', 'ima_flag', 'dkin_flag', 'ttv_flag', 'cb_flag']\n",
    "df[bool_col_names] = df[bool_col_names].astype(bool)\n",
    "\n",
    "col_to_remove = list((df.isnull().sum() / len(df)).loc[lambda p : p > 0.3].sort_values(ascending=False).index) # Drop columns with missing data >= 30%\n",
    "df = df.drop(col_to_remove, axis=1)\n",
    "df = df.drop(['pl_rade', 'pl_bmasse'], axis=1) # Drop highly correlated value\n",
    "df = df.drop(['discoverymethod'], axis=1) # Use one-hot encoded columns\n",
    "df = df.drop(['pl_name', 'hostname', 'disc_year', 'disc_locale', 'disc_facility', 'disc_telescope', 'disc_instrument', 'pl_bmassprov', 'st_metratio', 'ra_reflink'], axis=1) # Drop non numeric fields\n",
    "df = df.dropna() # Drop nan values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30758444-a4f7-410a-bb4b-c2c3b157bb9d",
   "metadata": {},
   "source": [
    "## Split dataset into Train and Test\n",
    "\n",
    "Among the planets that have the temperature data, split the dataset into two groups: train(70%) and test(30%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5043739d-5bb4-4b50-a290-07bcc41cb97e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1993, 58)\n",
      "y_train shape: (1993,)\n",
      "X_test shape: (855, 58)\n",
      "y_test shape: (855,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "temp_df = df[df['pl_eqt'].notnull()]\n",
    "temp_pred_df = df[df['pl_eqt'].isnull()]\n",
    "y = temp_df['pl_eqt']\n",
    "X = temp_df.drop('pl_eqt', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # random state is used to get the same output each time\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2376da-fd53-451c-889b-00577e068063",
   "metadata": {},
   "source": [
    "## Compute cost\n",
    "\n",
    "The model function for linear regression which is a function that maps from `X` (planet parameters) to `y` (planet effective temperature (K)) is represented as: $$f_{W,b}(X) = WX + b$$\n",
    "To train this linear regression model, you need to find the best $(W,b)$ parameters that fit your dataset.\n",
    "To compare how a one parameter $(W,b)$ is better or worse than another is by evaluating it with a cost function $J(W,b)$.\n",
    "\n",
    "\n",
    "The cost function is defined as: $$J(W,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{W,b}(X^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "where $m$ is the number of training examples, $f_{W,b}(X^{(i)})$ is the model's prediction of the planet's effective temperature and $y^{(i)}$ is the actual effective temperature from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af1ae3cd-d19e-4e47-a88e-182d6733f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, W, b): \n",
    "    \"\"\"\n",
    "    Computes the cost function for multi-variable linear regression.\n",
    "    \n",
    "    Arguments:\n",
    "    X: input matrix of shape (m, n) (Planet parameters) \n",
    "    y: output vector of shape (m, 1) (Actual effective temperature of the planet)\n",
    "    W: weight vector of shape (1, n)\n",
    "    b: bias scalar\n",
    "    \n",
    "    Returns\n",
    "    cost -- scalar value of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0] # number of training examples\n",
    "    f_Wb = np.dot(X, W.T) + b  # predicted output values\n",
    "    loss = (f_Wb - y)**2  # squared error \n",
    "    cost = np.sum(loss) / (2 * m)  # compute cost using mean squared error\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7784d88e-2114-4e0c-9852-ee2fb5702d84",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "The parameter that fits the data best will have the smallest cost $J(W,b)$. Gradient descent is used to find that smallest cost by stepping closer to the optimal value. \n",
    "The algorithm for gradient descent is:\n",
    "$$\\begin{align*}& \\text{repeat until convergence:} \\; \\lbrace \\newline\\; \n",
    "& \\phantom {0000} b := b -  \\alpha \\frac{\\partial J(W,b)}{\\partial b} \\newline\\; \n",
    "& \\phantom {0000} W := W -  \\alpha \\frac{\\partial J(W,b)}{\\partial W} \\; \\newline \n",
    "& \\rbrace\\end{align*}$$\n",
    "\n",
    "where parameters $W$, $b$ are both updated simultaniously and where:\n",
    "$$\\frac{\\partial J(W,b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{W,b}(X^{(i)}) - y^{(i)})$$\n",
    "\n",
    "$$\\frac{\\partial J(W,b)}{\\partial W}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{W,b}(X^{(i)}) -y^{(i)})X^{(i)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b4b2a66-f5b9-459a-9779-61dacc59d670",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, W, b): \n",
    "    \"\"\"\n",
    "    Compute the gradient of the cost function for multi-variable linear regression.\n",
    "\n",
    "    Arguments:\n",
    "    X: input matrix of shape (m, n) (Planet parameters) \n",
    "    y: output vector of shape (m, 1) (Actual effective temperature of the planet)\n",
    "    W: weight vector of shape (1, n)\n",
    "    b: bias scalar\n",
    "    \n",
    "    Returns\n",
    "    dj_dW: The gradient of the cost with respect to the parameters W\n",
    "    dj_db: The gradient of the cost with respect to the parameter b     \n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]  # number of training examples\n",
    "    f_Wb = np.dot(X, W.T) + b  # predicted output values\n",
    "    diff = f_Wb - y  # difference between predicted and true output values\n",
    "\n",
    "    dj_dW = np.dot(diff.T, X) / m  # gradient with respect to the weights\n",
    "    dj_db = np.sum(diff) / m  # gradient with respect to the bias\n",
    "\n",
    "    return dj_dW, dj_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf183847-ef57-4815-8c40-4a8c62331de8",
   "metadata": {},
   "source": [
    "## Batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaff96ad-7257-4b87-9d08-c3ec69440aec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, W_in, b_in, cost_function, gradient_function, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent\n",
    "\n",
    "    Arguments:\n",
    "    X: input matrix of shape (m, n)\n",
    "    y: output vector of shape (m, 1)\n",
    "    W_in: initial weight vector of shape (1, n)\n",
    "    b_in: initial bias scalar\n",
    "    cost_function: function to compute the cost given X, y, W, and b\n",
    "    gradient_function: function to compute the gradient of the cost function given X, y, W, and b\n",
    "    alpha: learning rate\n",
    "    num_iters: number of iterations to run gradient descent\n",
    "\n",
    "    Returns:\n",
    "    W: optimal weight vector of shape (1, n)\n",
    "    b: optimal bias scalar\n",
    "    \"\"\"\n",
    "\n",
    "    # An array to store cost J and w's at each iteration â€” primarily for graphing later\n",
    "    J_history = []\n",
    "    W_history = []\n",
    "    W = copy.deepcopy(W_in)  # Avoid modifying global w within function\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_dW, dj_db = gradient_function(X, y, W, b)  \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        W = W - alpha * dj_dW               \n",
    "        b = b - alpha * dj_db               \n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            cost = cost_function(X, y, W, b)\n",
    "            J_history.append(cost)\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0:\n",
    "            W_history.append(W)\n",
    "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")\n",
    "        \n",
    "    return W, b, J_history, W_history # Return W and J,W history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eee2c6e-4e96-439b-9195-e9ceeeb6bb65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 37821222289488555579822571520.00   \n"
     ]
    }
   ],
   "source": [
    "m,n = X.shape \n",
    "\n",
    "# initialize parameters\n",
    "initial_W = np.random.randn(1, n) * 0.000005\n",
    "initial_b = 0\n",
    "\n",
    "# gradient descent settings\n",
    "iterations = 1500\n",
    "alpha = 0.001\n",
    "\n",
    "W,b,J_history,W_history = gradient_descent(X_train, y_train, initial_W, initial_b, compute_cost, compute_gradient, alpha, iterations)\n",
    "print(\"W,b found by gradient descent:\", W, b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exo (venv)",
   "language": "python",
   "name": "exo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
